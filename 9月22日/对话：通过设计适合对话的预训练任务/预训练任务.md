传统预训练模型应用在对话存在以下三个问题
1） 上下游任务不匹配
2） 对话-回答二分类问题不能够表示话语内和话语间的联系
3） 现有模型倾向于根据回答与上下文的语义上的相似程度来选择最佳回答（可能一个更合适的回答却被模型判定为与Context有更低的相似度）

论文提出了UMS模型，设计了三个特殊的预训练任务：
    对多轮对话中语句的1.插入；2.删除；3.查找 
        1.插入
                模型不仅需要能区分语义不同的句子，还要能区分语义相关度的话语是否连续。
            从原始对话上下文中提取k个连续语句，然后随机选择一个要插入的语句，让模型去找该话语应该插入的位置，在每个语句前和最后一个语句后放置[INS]标记（表示目标语句可能的位置）
        2.删除
            选出一段包含k个语句的对话，通过语句插入插入一句随机的句子，模型需要从这k+1个句子中找到其中不正确的语句删除，每个语句之前插入[DEL]来标记位置 。
        3.查找
            前两个任务结束后，获得了一段顺序正确的对话，
            在对话中随机选一个句子，模型可以找到他的前一个句子。
            给定一段k个句子的对话，除了最后一句话，将剩下的句子打乱顺序，然后给每个乱序的句子插入[SRCH]标记。从最后一句开始向前查找句子。
    多任务训练