Bert是在通用语料库上进行与训练的，因此在微调时对于特定语料的监督是不够的。本文提出，在特定任务语料库上对BERT进行后期训练，预训练任务采用MLM和NSP

预训练关键点：
    1.由于目标是训练一个多轮对话的预训练模型，用[EOT]标记每一个句子的结束点，NSP任务是在一轮对话上进行的，让模型判断是不是一问一答。最终的损失MLM和NSP的损失相加。
模型使用：将Context（所有的语境）和一个Answer拼接。做一个2分类