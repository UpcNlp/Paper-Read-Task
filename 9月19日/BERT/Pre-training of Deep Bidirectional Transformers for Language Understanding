这是一篇最早提出Bert模型的文章，可在B站李沐论文讲解中找到该文章
BERT的全称为Bidirectional Encoder Representation from Transformers，是一个预训练的语言表征模型。它采用新的masked language model（MLM），能生成深度的双向语言表征。预训练后，只需要添加一个额外的
输出层进行fine-tune，就可以在各种各样的下游任务中取得state-of-the-art的表现。在这过程中并不需要对BERT进行任务特定的结构修改。
BERT利用MLM进行预训练并且采用深层的双向Transformer组件来构建整个模型，生成能融合左右上下文信息的深层双向语言表征。如图1所示。
论文主要介绍了两种尺寸的模型：
number of layers as L，
the hidden size as H，
the number of self-attention heads as A。 
BertBASE(L=12, H=768, A=12, Total Parameters=110M)
BertLARGE(L=24, H=1024,A=16, Total Parameters=340M)。
输入
每一个token（图1中的粉色块）对应的表征（图1中的黄色块），单词字典是采用WordPiece算法来进行构建的。
表征由三部分组成的，分别是对应的token，分割和位置 embeddings。如图2。
输出
C为分类token（[CLS]）对应最后一个Transformer的输出，[公式] 则代表其他token对应最后一个Transformer的输出。对于一些token级别的任务（如，序列标注和问答任务），就把[公式] 输入到额外的输出层中进行预测。对于一些句子级别的任务（如，自然语言推断和情感分类任务），就把C输入到额外的输出层中。


预训练
MLM是BERT能够不受单向语言模型所限制的原因。简单来说就是以15%的概率用mask token （[MASK]）随机地对每一个训练序列中的token进行替换，然后预测出[MASK]位置原有的单词。然而，由于[MASK]并不会出现在下游任务的微调（fine-tuning）阶段，因此预训练阶段和微调阶段之间产生了不匹配。故
在每一个训练序列中以15%的概率随机地选中某个token位置用于预测，假如是第i个token被选中，则会被替换成以下三个token之一：
1）80%的时候是[MASK]。如，my dog is hairy——>my dog is [MASK]。
2）10%的时候是随机的其他token。如，my dog is hairy——>my dog is apple。
3）10%的时候是原来的token（保持不变，个人认为是作为2）所对应的负类）。如，my dog is hairy——>my dog is hairy。
再用该位置对应的 T i 去预测原来的token，输入到全连接，然后用softmax输出每个token的概率，最后用交叉熵计算loss
​Next Sentence Prediction（NSP）
预测两个句子是否连在一起。具体的做法是：对于每一个训练样例，在语料库中挑选出句子A和句子B来组成，50%的时候句子B就是句子A的下一句（标注为IsNext），剩下50%的时候句子B是语料库中的随机句子（标注为NotNex
t）。接下来把训练样例输入到BERT模型中，用[CLS]对应的C信息去进行二分类的预测。
预训练任务的效果
No NSP:使用MLM但没有使用NSP。
LTR & No NSP：仅使用标准的Left-to-Right而不是使用MLM，同时没有使用NSP。
该试验证明了BERT 的深度双向性的重要性。见图3

 模型大小的影响
训练具有不同层数的 BERT 模型。一般来讲更大的模型可以提高实验的精度，但该实验证明了在对模型进行了充分的预训练的前提下，将模型缩小到极端大小也可以在小规模任务上带来很大的改进。见图4