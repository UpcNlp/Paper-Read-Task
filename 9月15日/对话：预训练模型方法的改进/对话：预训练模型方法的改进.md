开发了一种新的Transformer结构，即Poly-编码器,用于解决基于预训练模型做对话任务太慢的问题。它可以学习全局的而不是标记的自我注意特征。
    1.预训练模型：使用与BERT-base完全相同的架构从头开始预训练两个Transformer。其中一个使用与BERT-base类似的训练设置，在从维基百科和多伦多图书库中提取的1.5亿个[INPUT, LABEL]数据进行训练，而另一个则在从在线平台Reddit中提取的1.74亿个[INPUT, LABEL]的例子上进行训练，这是一个更适合对话的数据集。测试特定的对话数据集是否对任务有帮助
    2.输入表示：Context和Answer的串联
    3.训练策略：包括MLM任务，和Bert相同的上下句预测任务，作者自己设计的干扰任务：50%的可能替换Context中的句子为随机的一句话。

    文中叙述了两种先前的基于预训练模型的方法：
        Bi-encoder：用两个上面的训练策略训练好的预训练模型，为Context和Answer单独编码，使用点积进行两个特征的融合，得到最终候选Answer的分数。
        Cross-encoder：Context和Answer拼接进行编码，取最后一层输出的第一个向量，这种方法能够在上下文和候选之间执行self-attention，从而产生比Bi-encoder更丰富的提取机制。但是这种方法每一个候选Answer都要和Context拼接，太慢了。

    文中提出的方法：Poly-encoder
        Poly-encoder目标是从上述两个类型encoder中获得最佳结果。
        类似Bi-encoder：Poly-encoder对Context和Answer使用两个独立的Transformer编码，本文提出可以将所有的Answer向量进行缓存，Answer库是固定的，来提高速度。
        
        另外，由于Context通常特别长，所以对Context使用注意力机制进行编码，使用m个可学习上下文向量 ，对transformer的输出进行注意力编码，得到m个输出向量，使用候选Answer向量对m个输出向量  进行注意力编码，得到最终的Context向量

        将Context和Answer进行点积，得到最后的得分。

    结论：Poly-encoders的模型结构简单可行，其在Bi-encoders和Cross-encoders之间取得了很好的平衡，效果提高的同时，速度仍处在可接受的范围内。论文中在下游任务相似的语料中预训练模型是提升模型效果的一个不错的方法。（使用与任务匹配的语料进行预训练模型的预训练）
